{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2023-02-15 18:21:14,417 - topics - topicmanager initialized\n",
      "WARNING - 2023-02-15 18:21:14,421 - rigid_transformations - autolab_core not installed as catkin package, RigidTransform ros methods will be unavailable\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e7b1ac0d3e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgraspnetAPI\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraspGroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mROOT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import argparse\n",
    "import importlib\n",
    "import scipy.io as scio\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from graspnetAPI import GraspGroup\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'dataset'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "\n",
    "from graspnet import GraspNet, pred_decode\n",
    "from graspnet_dataset import GraspNetDataset\n",
    "from collision_detector import ModelFreeCollisionDetector\n",
    "from data_utils import CameraInfo, create_point_cloud_from_depth_image\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--checkpoint_path', required=True, help='Model checkpoint path')\n",
    "parser.add_argument('--num_point', type=int, default=20000, help='Point Number [default: 20000]')\n",
    "parser.add_argument('--num_view', type=int, default=300, help='View Number [default: 300]')\n",
    "parser.add_argument('--collision_thresh', type=float, default=0.01, help='Collision Threshold in collision detection [default: 0.01]')\n",
    "parser.add_argument('--voxel_size', type=float, default=0.01, help='Voxel Size to process point clouds before collision detection [default: 0.01]')\n",
    "cfgs = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_net():\n",
    "    # Init the model\n",
    "    net = GraspNet(input_feature_dim=0, num_view=cfgs.num_view, num_angle=12, num_depth=4,\n",
    "            cylinder_radius=0.05, hmin=-0.02, hmax_list=[0.01,0.02,0.03,0.04], is_training=False)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(cfgs.checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(\"-> loaded checkpoint %s (epoch: %d)\"%(cfgs.checkpoint_path, start_epoch))\n",
    "    # set model to eval mode\n",
    "    net.eval()\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_and_process_data(data_dir):\n",
    "    # load data\n",
    "    color = np.array(Image.open(os.path.join(data_dir, 'color_6.png')), dtype=np.float32) / 255.0\n",
    "    depth = np.array(Image.open(os.path.join(data_dir, 'depth_6.png')))\n",
    "    workspace_mask = np.array(Image.open(os.path.join(data_dir, 'workspace_mask.png')))\n",
    "    # meta = scio.loadmat(os.path.join(data_dir, 'meta.mat'))\n",
    "    # meta = scio.loadmat(os.path.join('//home/teamf/graspnet/graspnet-baseline/doc/example_data/', 'meta.mat'))\n",
    "    # intrinsic = meta['intrinsic_matrix']\n",
    "    # factor_depth = meta['factor_depth']\n",
    "    # intrinsic = np.array([[0.501, 0.801, 0.5],[ 0.506, -0.056, 0.065],[ -0.001, -0.001, -0.021]])/(-0.021)\n",
    "    intrinsic = np.array([[0.501, 0, 0.5],[ 0.0, -0.056, 0.065],[ -0.0, -0.0, -0.021]])/(-0.021)\n",
    "\n",
    "    factor_depth = 1000\n",
    "\n",
    "    # generate cloud\n",
    "    # camera = CameraInfo(1280.0, 720.0, intrinsic[0][0], intrinsic[1][1], intrinsic[0][2], intrinsic[1][2], factor_depth)\n",
    "    camera = CameraInfo(640, 480, intrinsic[0][0], intrinsic[1][1], intrinsic[0][2], intrinsic[1][2], factor_depth)\n",
    "    cloud = create_point_cloud_from_depth_image(depth, camera, organized=True) #shape=(480,640,3)\n",
    "\n",
    "    mask = (workspace_mask & (depth > 0)) #shape =(480,640)\n",
    "\n",
    "    cloud_masked = cloud[mask>0,:]\n",
    "    color_masked = color[mask>0,:]\n",
    "\n",
    "    print(cloud.max(), depth.max())\n",
    "\n",
    "    # sample points\n",
    "    if len(cloud_masked) >= cfgs.num_point:\n",
    "        idxs = np.random.choice(len(cloud_masked), cfgs.num_point, replace=False)\n",
    "    else:\n",
    "        idxs1 = np.arange(len(cloud_masked))\n",
    "        idxs2 = np.random.choice(len(cloud_masked), cfgs.num_point-len(cloud_masked), replace=True)\n",
    "        idxs = np.concatenate([idxs1, idxs2], axis=0)\n",
    "        \n",
    "    cloud_sampled = cloud_masked[idxs]\n",
    "    color_sampled = color_masked[idxs]\n",
    "\n",
    "    # convert data\n",
    "    cloud = o3d.geometry.PointCloud()\n",
    "    cloud.points = o3d.utility.Vector3dVector(cloud_masked.astype(np.float32))\n",
    "    cloud.colors = o3d.utility.Vector3dVector(color_masked.astype(np.float32))\n",
    "    end_points = dict()\n",
    "    cloud_sampled = torch.from_numpy(cloud_sampled[np.newaxis].astype(np.float32))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cloud_sampled = cloud_sampled.to(device)\n",
    "    end_points['point_clouds'] = cloud_sampled\n",
    "    end_points['cloud_colors'] = color_sampled\n",
    "\n",
    "    return end_points, cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grasps(net, end_points):\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        end_points = net(end_points)\n",
    "        grasp_preds = pred_decode(end_points)\n",
    "    gg_array = grasp_preds[0].detach().cpu().numpy()\n",
    "    gg = GraspGroup(gg_array)\n",
    "    return gg\n",
    "\n",
    "def collision_detection(gg, cloud):\n",
    "    mfcdetector = ModelFreeCollisionDetector(cloud, voxel_size=cfgs.voxel_size)\n",
    "    collision_mask = mfcdetector.detect(gg, approach_dist=0.05, collision_thresh=cfgs.collision_thresh)\n",
    "    gg = gg[~collision_mask]\n",
    "    return gg\n",
    "\n",
    "def vis_grasps(gg, cloud):\n",
    "    gg.nms()\n",
    "    gg.sort_by_score()\n",
    "    gg = gg[:50]\n",
    "    grippers = gg.to_open3d_geometry_list()\n",
    "    o3d.visualization.draw_geometries([cloud, *grippers])\n",
    "\n",
    "def demo(data_dir):\n",
    "    net = get_net()\n",
    "    end_points, cloud = get_and_process_data(data_dir)\n",
    "    gg = get_grasps(net, end_points)\n",
    "    if cfgs.collision_thresh > 0:\n",
    "        gg = collision_detection(gg, np.array(cloud.points))\n",
    "    vis_grasps(gg, cloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/teamf/rec_data'\n",
    "demo(data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
